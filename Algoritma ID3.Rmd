---
title: "Algoritma ID3 Dengan R"
date: "`r Sys.Date()`"
author: "Dona Dellila Doja Se-Institut Teknologi Statistika dan Bisnis Muhammadiyah"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
bibliography: references.bib
---

```{=html}
<style>
body{
text-align: justify}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Algoritma ID3

### Pengertian

Algoritma ID3 atau Iterative Dichotomiser 3 adalah sebuah metode yang digunakan untuk membuat pohon keputusan yang telah dikembangkan oleh J. Ross Quinlan sejak tahun 1986.

Algoritma ID3 melakukan pencarian secara menyeluruh pada semua kemungkinan pohon keputusan. Algoritma ID3 berusaha membangun decision tree (pohon keputusan) secara top-down (dari atas ke bawah)

### Strutur Pohon Keputusan

Struktur pohon keputusan terdiri dari root node (node akar), internal node (node cabang), dan leaf node. Proses klasifikasi dilakukan dari node paling atas dilanjutkan ke bawah melalui cabang-cabang sampai dihasilkan node daun (leafes) dimana node daun ini menunjukkan hasil akhir klasifikasi.

Pembentukan pohon klasifikasi dengan algoritma ID3 melalui dua langkah, yaitu :

  - Menghitung Nilai Entropy
  - Menghitung nilai information gain dari setiap variabel
     
# Tahapan Algoritma ID3
Tahapan Algoritma ID3 bisa dibagi menjadi 6 langkah yaitu :

### Menyiapkan Dataset 
     
  Dataset yang digunakan yaitu dataset yang berlabel nominal. Untuk algoritma ID3 menggunakan data jenis klasifikasi.   
     
     
### Menghitung Nilai Entropy
    
  Konsep Entropy digunakan untuk mengukur â€œseberapa informatifnyaâ€ sebuah node (yang biasanya disebut seberapa baiknya).
     
     Rumus Entropy :
     
           ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ (ğ‘ )= âˆ’ğ‘+ğ‘™ğ‘œğ‘”2ğ‘+ âˆ’ğ‘_ğ‘™ğ‘œğ‘”2ğ‘_
     
   Keterangan :
     
   S = Himpunan (dataset) Kasus.
  
  ğ‘_(+) = jumlah label yang bersolusi positif (mendukung)
  dibagi total kasus. 
  
  ğ‘_(âˆ’)= jumlah label yang bersolusi negative (tidak mendukung)
  dibagi total kasus.


### Menghitung Nilai Gain

  Information gain adalah kriteria pemisahan yang menggunakan pengukuran entropy.  Information gain digunakan untuk mengukur efektivitas suatu atribut dalam mengklasifikasikan data.
     
### Menentukan Root Node
  
  Root node atau node akar ditentukan berdasarkan nilai information gain yang telah di cari. Atribut dengan nilai information gain tertinggi akan menjadi root node nya. Atribut yang telah di pilih tidak diikutkan lagi ke perhitungan entropy dan information gain selanjutnya. 


### Membuat Node Cabang
  
  Node cabang di dapat dari perhitungan entropy dan information gain selanjutnya.


### Ulangi Langah 2-4 hingga membentuk sebuah pohon keputusan
  
  Mengulangi langkah 2-4 hingga membentuk pohon keputusan.

#  Eksperimen Algoritma ID3
### Library
```{r}
library(dplyr)
```

### Loading Data set

sebagai contoh kita akan menggunakan dataset iris 

```{r}
df <- iris
#df$Species<- unclass(df$Species)

```

### Menghitung Nilai Entropy 

Nilai entropy pada setiap kolom/fitur dapat dihitung sebagai berikut

```{r}
entropy <- function(target) {
  freq <- table(target)/length(target)
  # Vektorisasi kolom dataframe
  vec <- as.data.frame(freq)[,2]
  #drop 0 to avoid NaN resulting from log2
  vec<-vec[vec>0]
  # Menghitung Nilai Entropy
  -sum(vec * log2(vec))}
# Menghitung Nilai Entropy Kolom Species
print(entropy(df$Species))
```
```{r}
# Menghitung Nilai Entropy Kolom Sepal.Width
print(entropy(df$Sepal.Width))
```

### Nilai Information Gain

Menghitung nilai information gain dari setipa fitur/ kolom dapat dilakukan sebagai berikut 

```{r}
IG_numeric<-function(data, feature, target, bins=4) {
  #Hapus baris di mana fiturnya adalah NA
  data<-data[!is.na(data[,feature]),]
  #Menghitung entropi untuk induk(label data)
  e0<-entropy(data[,target])
  
  data$cat<-cut(data[,feature], breaks=bins, labels=c(1:bins))
  
  #gunakan dplyr untuk menghitung e dan p untuk setiap nilai fitur
  dd_data <- data %>% group_by(cat) %>% summarise(e=entropy(get(target)), 
                 n=length(get(target)),
                 min=min(get(feature)),
                 max=max(get(feature))
                 )
  
  #hitung p untuk setiap nilai fitur
  dd_data$p<-dd_data$n/nrow(data)
  #menghitung IG
  IG<-e0-sum(dd_data$p*dd_data$e)
  
  return(IG)
}

IG_numeric(df, "Sepal.Length", "Species", bins=5)
```

### Data Frame

Data Frame untuk Nilai Entropy dan Information Gain Setiap Kolom yang diurutkan sebagai berikut

```{r}
Fitur_Exploration <- function(df, bin){
  E <- numeric()
  for (i in 1:ncol(df)){
    nama<-names(df)[i]
    E[i]<-entropy(df[,nama])
    }
  
  ig <- numeric()
  kol=ncol(df)-1
  for (i in 1:kol){
    ig[i]<-IG_numeric(df, names(df)[i], names(df)[ncol(df)], bins=bin)
  }
  ig[ncol(df)]<-0 #Masih dicek lagi
  Column_Name <- names(df)
  Entropy <- E
  IG <- ig
  df_E <- data.frame(Column_Name, Entropy, IG)
  df_E_sort <- df_E[order(-IG),]
  return(df_E_sort)
} 

Fitur_Exploration(df,5)
```
# membuat Pohon Keputusan

```{r}
data("iris")
names(iris)
table(iris$Species)
head(iris)
```
```{r}
library(psych)
describe(iris)
```
```{r}
library(rpart)
library(rpart.plot)
trees<-rpart(Species ~., data = iris, method = 'class')
rpart.plot(trees)
```


# Referensi

1. https://rpubs.com/Eliyanto29/Entropy_and_Information_G


