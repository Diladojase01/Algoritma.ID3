---
title: "Algoritma ID3 Dengan R"
date: "`r Sys.Date()`"
author: "Dona Dellila Doja Se-Institut Teknologi Statistika dan Bisnis Muhammadiyah"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
bibliography: references.bib
---

```{=html}
<style>
body{
text-align: justify}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Algoritma ID3

#### Pengertian
Algoritma ID3 atau Iterative Dichotomiser 3 adalah sebuah metode yang digunakan untuk membuat pohon keputusan yang telah dikembangkan oleh J. Ross Quinlan sejak tahun 1986.

Algoritma ID3 melakukan pencarian secara menyeluruh pada semua kemungkinan pohon keputusan. Algoritma ID3 berusaha membangun decision tree (pohon keputusan) secara top-down (dari atas ke bawah)

#### Strutur Pohon Keputusan
Struktur pohon keputusan terdiri dari root node (node akar), internal node (node cabang), dan leaf node. Proses klasifikasi dilakukan dari node paling atas dilanjutkan ke bawah melalui cabang-cabang sampai dihasilkan node daun (leafes) dimana node daun ini menunjukkan hasil akhir klasifikasi.

Pembentukan pohon klasifikasi dengan algoritma ID3 melalui dua langkah, yaitu :

  - Menghitung Nilai Entropy
  - Menghitung nilai information gain dari setiap variabel

# Tahapan Algoritma ID3
Tahapan Algoritma ID3 bisa dibagi menjadi 6 langkah yaitu :

#### Menyiapkan Dataset 
  Dataset yang digunakan yaitu dataset yang berlabel nominal. Untuk algoritma ID3 menggunakan data jenis klasifikasi.   
     
#### Menghitung Nilai Entropy
  Konsep Entropy digunakan untuk mengukur â€œseberapa informatifnyaâ€ sebuah node (yang biasanya disebut seberapa baiknya).
     
     Rumus Entropy :
     
    ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ (ğ‘ )= âˆ’ğ‘(+)ğ‘™ğ‘œğ‘”2ğ‘(+) âˆ’ğ‘(-)ğ‘™ğ‘œğ‘”2ğ‘(-)
     
   Keterangan :
     
   S = Himpunan (dataset) Kasus.
  
  ğ‘(+) = jumlah label yang bersolusi positif (mendukung)
  dibagi total kasus. 
  
  ğ‘(âˆ’) = jumlah label yang bersolusi negative (tidak mendukung)
  dibagi total kasus.

#### Menghitung Nilai Gain
  Information gain adalah kriteria pemisahan yang menggunakan pengukuran entropy.  Information gain digunakan untuk mengukur efektivitas suatu atribut dalam mengklasifikasikan data.
  
#### Menentukan Root Node
  Root node atau node akar ditentukan berdasarkan nilai information gain yang telah di cari. Atribut dengan nilai information gain tertinggi akan menjadi root node nya. Atribut yang telah di pilih tidak diikutkan lagi ke perhitungan entropy dan information gain selanjutnya. 

#### Membuat Node Cabang
  Node cabang di dapat dari perhitungan entropy dan information gain selanjutnya.

#### Ulangi Langah 2-4 Hingga Membentuk Sebuah Pohon Keputusan
  Mengulangi langkah 2-4 hingga membentuk pohon keputusan.

#  Eksperimen Algoritma ID3

#### Library
```{r}
library(dplyr)
```

#### Loading Dataset
sebagai contoh kita akan menggunakan dataset iris 
```{r}
library(data.tree)
View(iris)
df <- iris
#df$Species<- unclass(df$Species)
```

#### Menghitung Nilai Entropy 
Nilai entropy pada setiap kolom dapat dihitung sebagai berikut
```{r}
entropy <- function(target) {
  freq <- table(target)/length(target)
  # Vektorisasi kolom dataframe
  vec <- as.data.frame(freq)[,2]
  #drop 0 to avoid NaN resulting from log2
  vec<-vec[vec>0]
  # Menghitung Nilai Entropy
  -sum(vec * log2(vec))}
```
Menghitung Nilai Entropy kolom Species
```{r}
print(entropy(df$Species))
```
Menghitung Nilai Entropy kolom sepal.lenght
```{r}
print(entropy(df$Sepal.Length))
```
Menghitung Nilai Entropy kolom sepal.width
```{r}
print(entropy(df$Sepal.Width))
```
Menghitung Nilai Entropy kolom petal.length
```{r}
print(entropy(df$Petal.Length))
```
Menghitung Nilai Entropy kolom petal.width
```{r}
print(entropy(df$Petal.Width))
```
#### Nilai Information Gain
Menghitung nilai information gain dari setiap kolom dapat dilakukan sebagai berikut :
```{r}
#mengembalikan IG untuk variabel numerik
IG_numeric<-function(data, feature, target, bins=4) {
  #Hapus baris di mana fitur adalah NA
  data<-data[!is.na(data[,feature]),]
  #menghitung entropi untuk induknya
  e0<-entropy(data[,target])
  
  data$cat<-cut(data[,feature], breaks=bins, labels=c(1:bins))
  
  #gunakan dplyr untuk menghitung e dan p untuk setiap nilai kolom
  dd_data <- data %>% group_by(cat) %>% summarise(e=entropy(get(target)), 
                 n=length(get(target)),
                 min=min(get(feature)),
                 max=max(get(feature))
                 )
  
  #menghitung p untuk setiap nilai kolom
  dd_data$p<-dd_data$n/nrow(data)
  #Menghitung IG
  IG<-e0-sum(dd_data$p*dd_data$e)
  
  return(IG)
}
```
Menghitung nilai information gain untuk kolom Sepal.Length
```{r}
IG_numeric(iris, "Sepal.Length", "Species", bins=5)
```
Menghitung nilai information gain untuk kolom Sepal.Width
```{r}
IG_numeric(iris, "Sepal.Width", "Species", bins=5)
```
Menghitung nilai information gain untuk kolom Petal.Length
```{r}
IG_numeric(iris, "Petal.Length", "Species", bins=5)
```
Menghitung nilai information gain untuk kolom Petal.Width
```{r}
IG_numeric(iris, "Petal.Width", "Species", bins=5)
```
#### Data Frame
Data Frame untuk Nilai Entropy dan Information Gain Setiap Kolom yang diurutkan sebagai berikut
```{r}
Fitur_Exploration <- function(df, bin){
  E <- numeric()
  for (i in 1:ncol(df)){
    nama<-names(df)[i]
    E[i]<-entropy(df[,nama])
    }
  
  ig <- numeric()
  kol=ncol(df)-1
  for (i in 1:kol){
    ig[i]<-IG_numeric(df, names(df)[i], names(df)[ncol(df)], bins=bin)
  }
  ig[ncol(df)]<-0 #Masih dicek lagi
  Column_Name <- names(df)
  Entropy <- E
  IG <- ig
  df_E <- data.frame(Column_Name, Entropy, IG)
  df_E_sort <- df_E[order(-IG),]
  return(df_E_sort)
} 

Fitur_Exploration(df,5)
```
# Membuat Pohon Keputusan dila
```{r}
library(rpart)
library(rpart.plot)
trees<-rpart(Species ~., data = iris, method = 'class')
rpart.plot(trees)
```

# Referensi
1. https://rpubs.com/Eliyanto29/Entropy_and_Information_G
2. https://rpubs.com/gluc/ID3
3. https://rstudio-pubs-static.s3.amazonaws.com/455435_30729e265f7a4d049400d03a18e218db.html
4. https://medium.com/analytics-vidhya/visualizing-decision-tree-with-r-774f58ac23c